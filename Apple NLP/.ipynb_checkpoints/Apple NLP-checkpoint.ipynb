{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Connect to webhose, get your API key and do the following, to retrieve the dataset.\n",
    "import json\n",
    "# import word2vec\n",
    "import gensim, operator,sys\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import watson_developer_cloud\n",
    "import pprint\n",
    "import webhoseio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "webhoseio.config(token='8e242357-4a54-4645-a4ef-5ce38b0b9c7d ')\n",
    "query_params = {\n",
    "    'q': 'organization:Apple',\n",
    "    'ts': '1511056980199',\n",
    "    'sort': 'crawled'\n",
    "}\n",
    "\n",
    "results = webhoseio.query('filterWebContent', query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anly_ner.json', 'w', encoding='utf8') as outfile:\n",
    "    outfile.write(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile =  open('anly_ner.json','r')\n",
    "mydata = json.loads(myfile.read())\n",
    "\n",
    "titles = set()\n",
    "for post in mydata['posts']:\n",
    "    if post['title']:\n",
    "        titles.add(post['title'])\n",
    "\n",
    "titles = list(titles)\n",
    "mydata_titles = ' '.join(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import Features, EntitiesOptions\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(iam_apikey='tkYa9Q8orVp7Mn3zrkA7DOh0Ic-VfgZL2a7kHWqjrn-Y',\n",
    "  version=\"2017-11-20\")\n",
    "response = natural_language_understanding.analyze(\n",
    "   text = mydata_titles,\n",
    "  features=Features(entities=EntitiesOptions())\n",
    ")\n",
    "# #print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "#first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens  = [word for sent in nltk.sent_tokenize(text) \\\n",
    "              for word in tokenizer.tokenize(sent) if word not in stopwords]\n",
    "    lmtzr   = WordNetLemmatizer()\n",
    "\n",
    "    filtered_tokens = []\n",
    "   # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "         if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token.lower())\n",
    "    \n",
    "    stems = [lmtzr.lemmatize(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def clstr_lda(num_topics, stories):\n",
    "    \"\"\"\n",
    "    :rtype: object\n",
    "    \"\"\"\n",
    "    n_top_words = 15\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "    tf = tf_vectorizer.fit_transform(stories)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=200,\n",
    "                                    learning_method='online', learning_offset=9.,\n",
    "                                    random_state=1)\n",
    "    lda.fit(tf)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    #print(\"\\nLDA Topics:\")\n",
    "    # print top topic words\n",
    "    topics = dict()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topics[topic_idx] = [tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" | \".join([tf_feature_names[i]\n",
    "                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    #print()\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "industrials | fall | roundup | mexico | tariff | measure | may | applecare | headphone | jack | galaxy | note | samsung | online | four\n",
      "Topic #1:\n",
      "iphone | apple | ride | subway | air | download | 200mb | today | best | deal | company | rocketman | free | city | future\n",
      "Topic #2:\n",
      "apple | year | itunes | new | end | apps | mac | next | bring | computer | era | feature | later | plan | tech\n",
      "Topic #3:\n",
      "apple | itunes | apps | limit | tracking | third | party | week | report | android | main | reportedly | kid | donepezil | card\n",
      "Topic #4:\n",
      "wwdc | apple | expect | iphone | china | dominate | looking | xiaomi | monday | keynote | redmi | say | one | conference | wither\n",
      "Topic #5:\n",
      "apple | new | wwdc | macos | news | york | google | saatchi | tvt | effie | double | entendre | show | year | expect\n",
      "Topic #6:\n",
      "app | apple | download | mb | limit | store | io | superfast | prepare | 5g | ridiculous | bump | ipod | touch | still\n",
      "Topic #7:\n",
      "apple | mac | ipad | di | closer | ever | poised | bring | back | july | going | developer | wwdc | service | viola\n"
     ]
    }
   ],
   "source": [
    "myfile =  open('anly_ner.json','r')\n",
    "mydata = json.loads(myfile.read())\n",
    "\n",
    "texts = list()\n",
    "for post in mydata['posts']:\n",
    "    texts.append(post['title'])\n",
    "\n",
    "topics = clstr_lda(8, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('results.txt','w')\n",
    "for topic_idx, topic in topics.items():\n",
    "    output.write(\"Topic #%d: \\n\" % topic_idx)\n",
    "    output.write(\" | \".join(topic) + '\\n')\n",
    "        \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_taxonomy = {\n",
    "    \"offers\":\n",
    "    {\n",
    "        \"free\": \"shipping trial product\",\n",
    "        \"deal\": \"discount countdown limited \",\n",
    "        \"unlimited\": \"stock warranty buy\"\n",
    "    },\n",
    "    \"companies\":\n",
    "    {\n",
    "        \"amazon\": \"shopping order department\",\n",
    "        \"facebook\": \"likes social advertisement\",\n",
    "        \"google\": \"maps search doodle\"\n",
    "    },\n",
    "    \"returns\":\n",
    "    {\n",
    "        \"shipped\": \"free checkout prime\",\n",
    "        \"cashback\": \"credit cash days\",\n",
    "        \"balance\": \"card money transfer\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "import gensim,operator\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path =  \"watson_nlp_model/\"\n",
    "\n",
    "def load_word2vec_model(modelName, modelFile, flagBin):\n",
    "    print('Loading ' + modelName + ' model...')\n",
    "    model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin)\n",
    "    print('Finished loading ' + modelName + ' model...')\n",
    "    return model\n",
    "model_word2vec = load_word2vec_model('word2Vec', 'GoogleNews-vectors-negative300.bin', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_check(vectors,words):\n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "            \n",
    "    return output\n",
    "\n",
    "#function calculating similarity between two strings :\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    if len(s1words) > 0 and len(s2words) > 0:\n",
    "        return vectors.n_similarity(s1words, s2words)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    target = titles[i]\n",
    "    scores = []\n",
    "    subtitles = titles.copy()\n",
    "    subtitles.pop(i)\n",
    "    for title in subtitles:\n",
    "        scores.append((title,calc_similarity(target, title, model_word2vec)))\n",
    "    \n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print('Target Title: ', target)\n",
    "    print('----------------------------------------------------------')\n",
    "    for j in range(10):\n",
    "        print(sorted_scores[j][0])\n",
    "        \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_similarity(input1, input2, vectors):\n",
    "    term_vectors = [np.zeros(300), np.zeros(300)]\n",
    "    terms = [input1, input2]\n",
    "        \n",
    "    for index, term in enumerate(terms):\n",
    "        for i, t in enumerate(term.split(' ')):\n",
    "            try:\n",
    "                term_vectors[index] += vectors[t]\n",
    "            except:\n",
    "                term_vectors[index] += 0\n",
    "        \n",
    "    result = (1 - spatial.distance.cosine(term_vectors[0], term_vectors[1]))\n",
    "    if result is 'nan':\n",
    "        result = 0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function checks whether the input words are present in the vocabulary for the model\n",
    "def vocab_check(vectors, words):\n",
    "    \n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes an input string, runs similarity for each item in topic_taxonomy, sorts and returns top 3 results\n",
    "def classify_topics(input, vectors):\n",
    "    feed_score = dict()\n",
    "    for key, value in topic_taxonomy.items():\n",
    "        max_value_score = dict()\n",
    "        for label, keywords in value.items():\n",
    "            max_value_score[label] = 0\n",
    "            topic = (key + ' ' + keywords).strip()\n",
    "            max_value_score[label] += float(calc_similarity(input, topic, vectors))\n",
    "            \n",
    "        sorted_max_score = sorted(max_value_score.items(), key=operator.itemgetter(1), reverse=True)[0]\n",
    "        feed_score[sorted_max_score[0]] = sorted_max_score[1]\n",
    "    return sorted(feed_score.items(), key=operator.itemgetter(1), reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('classified topics.txt','w')\n",
    "for post in mydata['posts']:\n",
    "        output.write(str(str(post[\"title\"]).encode(encoding='ascii',errors='ignore')))\n",
    "        output.write('\\n')\n",
    "        try:\n",
    "            output.write(str(classify_topics(post[\"title\"], model_word2vec))+'\\n\\n')\n",
    "        except ZeroDivisionError:\n",
    "            output.write('0')\n",
    "        \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----Done-----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
